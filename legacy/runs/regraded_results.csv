batch_id,run_number,model,timestamp,passed,assertions_passed,assertions_total,metrics_count,benchmarks_found,tokens,conversation_span_ms,tool_calls,tool_errors,mcp_calls,mcp_errors,execute_calls,execute_errors,error_message,llm_time_ms,tool_time_ms,turns
2025_12_01_12_19,1,claude-haiku-4-5,2025-12-01T12:19:28.370937,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",64682,21948.85,5,0,1,0,4,0,,20636.04,1312.81,6
2025_12_01_12_19,2,claude-haiku-4-5,2025-12-01T12:19:57.641649,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",50154,18601.1,5,0,0,0,5,0,,17624.84,976.26,6
2025_12_01_12_19,3,claude-haiku-4-5,2025-12-01T12:20:17.548368,False,11,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",40858,17964.68,4,0,0,0,4,0,"Metric arc_challenge has value 44.5, expected 48.5",16902.6,1062.08,5
2025_12_01_12_19,4,claude-haiku-4-5,2025-12-01T12:20:37.790347,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",50174,17577.17,5,0,0,0,5,0,,16548.29,1028.88,6
2025_12_01_12_19,5,claude-haiku-4-5,2025-12-01T12:20:56.613369,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",40435,17504.51,4,0,0,0,4,0,,16477.22,1027.29,5
2025_12_01_12_19,6,claude-haiku-4-5,2025-12-01T12:21:16.477605,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",50104,17404.03,5,0,0,0,5,0,,16369.64,1034.39,6
2025_12_01_12_19,7,claude-haiku-4-5,2025-12-01T12:21:36.233702,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",59073,21611.79,6,1,0,0,6,1,,19897.85,1713.94,7
2025_12_01_12_19,8,claude-haiku-4-5,2025-12-01T12:21:59.120751,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",71805,27133.93,7,1,0,0,7,1,,26013.53,1120.4,8
2025_12_01_12_19,9,claude-haiku-4-5,2025-12-01T12:22:28.587431,False,11,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",51087,20286.89,5,0,0,0,5,0,"Metric arc_challenge has value 44.5, expected 48.5",18193.42,2093.47,6
2025_12_01_12_19,10,claude-haiku-4-5,2025-12-01T12:22:51.148247,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",60776,20864.92,6,0,0,0,6,0,,19689.96,1174.96,7
2025_12_01_12_23,1,claude-sonnet-4-5,2025-12-01T12:23:25.768965,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",63035,43973.26,6,0,0,0,6,0,,42857.18,1116.08,7
2025_12_01_12_23,2,claude-sonnet-4-5,2025-12-01T12:24:17.992130,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",52100,285047.55,5,0,0,0,5,0,,283761.13,1286.42,6
2025_12_01_12_23,3,claude-sonnet-4-5,2025-12-01T12:29:12.136319,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",50526,33436.92,5,0,0,0,5,0,,32362.44,1074.48,6
2025_12_01_12_23,4,claude-sonnet-4-5,2025-12-01T12:29:53.885448,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",52541,43004.52,5,0,0,0,5,0,,41928.31,1076.21,6
2025_12_01_12_23,5,claude-sonnet-4-5,2025-12-01T12:30:39.135053,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",79209,46898.88,6,0,1,0,5,0,,45168.43,1730.45,7
2025_12_01_12_23,6,claude-sonnet-4-5,2025-12-01T12:31:28.251699,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",50160,32104.64,5,0,0,0,5,0,,31029.76,1074.88,6
2025_12_01_12_23,7,claude-sonnet-4-5,2025-12-01T12:32:03.507170,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",39963,29961.8,4,0,0,0,4,0,,27866.07,2095.73,5
2025_12_01_12_23,8,claude-sonnet-4-5,2025-12-01T12:32:34.741450,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",77475,46018.8,7,0,0,0,7,0,,44923.9,1094.9,8
2025_12_01_12_23,9,claude-sonnet-4-5,2025-12-01T12:33:25.048619,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",52323,43924.08,5,0,0,0,5,0,,42795.88,1128.2,6
2025_12_01_12_23,10,claude-sonnet-4-5,2025-12-01T12:34:17.308773,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",50393,41624.03,5,0,0,0,5,0,,40553.28,1070.75,6
2025_12_01_12_40,1,openai/gpt-oss-120b,2025-12-01T12:40:26.731554,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",125152,10278.04,10,0,1,0,9,0,,8868.11,1409.93,11
2025_12_01_12_40,2,openai/gpt-oss-120b,2025-12-01T12:40:43.285354,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",38396,2757.6,3,0,0,0,3,0,,2542.76,214.84,4
2025_12_01_12_40,3,openai/gpt-oss-120b,2025-12-01T12:40:47.230541,False,6,23,1,,54925,8916.69,7,1,1,0,6,1,"Metrics list has 1 entries, expected at least 11",8019.83,896.86,8
2025_12_01_12_40,4,openai/gpt-oss-120b,2025-12-01T12:40:58.468845,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",36943,5367.43,6,0,0,0,6,0,,4348.51,1018.92,7
2025_12_01_12_40,5,openai/gpt-oss-120b,2025-12-01T12:41:05.071469,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",70848,5394.03,5,0,0,0,5,0,,5109.81,284.22,6
2025_12_01_12_40,6,openai/gpt-oss-120b,2025-12-01T12:41:11.653518,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",52174,3475.62,4,0,0,0,4,0,,3240.92,234.7,5
2025_12_01_12_40,7,openai/gpt-oss-120b,2025-12-01T12:41:16.331705,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",221417,19587.95,23,2,0,0,23,2,,17530.5,2057.45,24
2025_12_01_12_40,8,openai/gpt-oss-120b,2025-12-01T12:41:43.216539,False,6,23,1,,33942,3815.76,4,0,1,0,3,0,"Metrics list has 1 entries, expected at least 11",3485.2,330.56,5
2025_12_01_12_40,9,openai/gpt-oss-120b,2025-12-01T12:41:48.394331,False,4,23,0,,27774,3567.28,4,0,1,0,3,0,"Task type is 'multiple-choice-qa', expected 'text-generation'",3270.58,296.7,5
2025_12_01_12_40,10,openai/gpt-oss-120b,2025-12-01T12:41:53.396268,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",57354,4976.31,4,0,0,0,4,0,,4709.13,267.18,5
2025_12_01_12_42,1,moonshotai/Kimi-K2-Instruct-0905,2025-12-01T12:42:10.033487,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",74969,25781.79,7,0,1,0,6,0,,24371.36,1410.43,8
2025_12_01_12_42,2,moonshotai/Kimi-K2-Instruct-0905,2025-12-01T12:42:38.074674,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",63085,22004.35,6,0,1,0,5,0,,20773.11,1231.24,7
2025_12_01_12_42,3,moonshotai/Kimi-K2-Instruct-0905,2025-12-01T12:43:01.322106,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",48078,20797.28,6,0,0,0,6,0,,19717.58,1079.7,7
2025_12_01_12_42,4,moonshotai/Kimi-K2-Instruct-0905,2025-12-01T12:43:28.965046,False,2,23,0,,88798,58443.81,9,0,1,0,8,0,"Model name is 'allenai/OLMo-7B', expected one of ['OLMo 7B', 'OLMo-7B']",57838.53,605.28,10
2025_12_01_12_42,5,moonshotai/Kimi-K2-Instruct-0905,2025-12-01T12:44:30.280122,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",71419,51794.96,8,1,0,0,8,1,,50302.59,1492.37,9
2025_12_01_12_42,6,moonshotai/Kimi-K2-Instruct-0905,2025-12-01T12:45:30.126536,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",63504,23847.18,6,0,1,0,5,0,,22537.42,1309.76,7
2025_12_01_12_42,7,moonshotai/Kimi-K2-Instruct-0905,2025-12-01T12:45:55.980416,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",39947,22195.63,5,0,0,0,5,0,,21107.02,1088.61,6
2025_12_01_12_42,8,moonshotai/Kimi-K2-Instruct-0905,2025-12-01T12:46:20.051074,False,0,23,0,,2647,1657.22,1,0,0,0,1,0,Output file 'olmo_7b_evaluations.yaml' not found,1657.22,0.0,1
2025_12_01_12_42,9,moonshotai/Kimi-K2-Instruct-0905,2025-12-01T12:46:23.015091,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",102294,58975.55,11,1,0,0,11,1,,57363.82,1611.73,12
2025_12_01_12_42,10,moonshotai/Kimi-K2-Instruct-0905,2025-12-01T12:47:29.687837,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",128593,68048.93,13,0,0,0,13,0,,65734.67,2314.26,14
2025_12_01_12_49,1,zai-org/GLM-4.6,2025-12-01T12:49:13.381211,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",88821,9311.74,8,0,1,0,7,0,,7985.62,1326.12,9
2025_12_01_12_49,2,zai-org/GLM-4.6,2025-12-01T12:49:23.946962,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",65200,7920.73,8,2,0,0,8,2,,6761.66,1159.07,9
2025_12_01_12_49,3,zai-org/GLM-4.6,2025-12-01T12:49:33.827884,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",76749,7800.43,7,0,1,0,6,0,,6429.3,1371.13,8
2025_12_01_12_49,4,zai-org/GLM-4.6,2025-12-01T12:49:42.886547,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",139735,12734.18,12,1,1,0,11,1,,10455.06,2279.12,13
2025_12_01_12_49,5,zai-org/GLM-4.6,2025-12-01T12:49:57.730978,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",76969,8551.97,7,0,1,0,6,0,,7262.45,1289.52,8
2025_12_01_12_49,6,zai-org/GLM-4.6,2025-12-01T12:50:12.703888,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",58214,6727.32,7,1,0,0,7,1,,5586.03,1141.29,8
2025_12_01_12_49,7,zai-org/GLM-4.6,2025-12-01T12:50:20.689182,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",58554,7398.0,7,0,0,0,7,0,,6324.29,1073.71,8
2025_12_01_12_49,8,zai-org/GLM-4.6,2025-12-01T12:50:29.355198,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",83658,8072.17,8,0,1,0,7,0,,6749.29,1322.88,9
2025_12_01_12_49,9,zai-org/GLM-4.6,2025-12-01T12:50:39.575263,False,22,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",49568,7222.53,6,0,0,0,6,0,Missing source or source URL,6048.93,1173.6,7
2025_12_01_12_49,10,zai-org/GLM-4.6,2025-12-01T12:50:48.062700,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",66716,8803.64,8,0,0,0,8,0,,7292.99,1510.65,9
2025_12_01_12_51,1,grok-4-fast-non-reasoning,2025-12-01T12:51:40.104000,False,6,23,1,,36228,17222.51,4,1,1,0,3,1,"Metrics list has 1 entries, expected at least 11",16780.17,442.34,5
2025_12_01_12_51,2,grok-4-fast-non-reasoning,2025-12-01T12:51:58.723162,False,7,23,11,,34801,16208.98,4,1,1,0,3,1,Only found 0 expected benchmarks: set(),15811.8,397.18,5
2025_12_01_12_51,3,grok-4-fast-non-reasoning,2025-12-01T12:52:16.971769,False,11,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",63378,28657.48,8,1,0,0,8,1,"Metric arc_challenge has value 46.5, expected 48.5",26595.38,2062.1,9
2025_12_01_12_51,4,grok-4-fast-non-reasoning,2025-12-01T12:52:47.645875,False,6,23,1,,43274,17641.49,5,2,1,0,4,2,"Metrics list has 1 entries, expected at least 11",14184.46,3457.03,6
2025_12_01_12_51,5,grok-4-fast-non-reasoning,2025-12-01T12:53:06.530061,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",31515,8235.88,4,0,0,0,4,0,,6920.49,1315.39,5
2025_12_01_12_51,6,grok-4-fast-non-reasoning,2025-12-01T12:53:16.843919,False,0,23,0,,208744,146628.95,16,9,1,0,15,9,Output file 'olmo_7b_evaluations.yaml' not found,51633.86,94995.09,17
2025_12_01_12_51,7,grok-4-fast-non-reasoning,2025-12-01T12:55:47.687758,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",241724,133719.47,15,7,1,0,14,7,,42546.16,91173.31,16
2025_12_01_12_51,8,grok-4-fast-non-reasoning,2025-12-01T12:58:10.629830,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",46526,17660.71,6,2,1,0,5,2,,17066.2,594.51,7
2025_12_01_12_51,9,grok-4-fast-non-reasoning,2025-12-01T12:58:30.265388,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",51378,17966.3,7,2,0,0,7,2,,16820.22,1146.08,8
2025_12_01_12_51,10,grok-4-fast-non-reasoning,2025-12-01T12:58:50.429578,False,7,23,11,,300041,274163.19,18,10,1,0,17,10,Only found 0 expected benchmarks: set(),92520.1,181643.09,19
2025_12_01_13_04,1,MiniMaxAI/MiniMax-M2,2025-12-01T13:04:30.971099,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",41507,30448.94,5,0,0,0,5,0,,29385.72,1063.22,6
2025_12_01_13_04,2,MiniMaxAI/MiniMax-M2,2025-12-01T13:05:04.523357,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",78088,47508.32,7,0,1,0,6,0,,45913.34,1594.98,8
2025_12_01_13_04,3,MiniMaxAI/MiniMax-M2,2025-12-01T13:05:54.928708,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",41497,31134.14,5,0,0,0,5,0,,28721.55,2412.59,6
2025_12_01_13_04,4,MiniMaxAI/MiniMax-M2,2025-12-01T13:06:27.990625,False,6,23,1,,66840,74023.42,9,1,0,0,9,1,"Metrics list has 1 entries, expected at least 11",44840.19,29183.23,10
2025_12_01_13_04,5,MiniMaxAI/MiniMax-M2,2025-12-01T13:07:44.748230,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",55956,38576.1,8,1,0,0,8,1,,34455.22,4120.88,9
2025_12_01_13_04,6,MiniMaxAI/MiniMax-M2,2025-12-01T13:08:30.355109,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",65781,37255.09,10,2,0,0,10,2,,35969.93,1285.16,11
2025_12_01_13_04,7,MiniMaxAI/MiniMax-M2,2025-12-01T13:09:09.584726,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",65581,39999.53,6,0,1,0,5,0,,38349.92,1649.61,7
2025_12_01_13_04,8,MiniMaxAI/MiniMax-M2,2025-12-01T13:09:56.389412,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",50469,34016.69,6,0,0,0,6,0,,32910.45,1106.24,7
2025_12_01_13_04,9,MiniMaxAI/MiniMax-M2,2025-12-01T13:10:32.336600,False,11,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",61453,37486.6,7,0,0,0,7,0,"Metric arc_challenge has value 44.5, expected 48.5",35263.32,2223.28,8
2025_12_01_13_04,10,MiniMaxAI/MiniMax-M2,2025-12-01T13:11:12.281734,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",42059,32123.08,5,0,0,0,5,0,,31003.02,1120.06,6
2025_12_01_13_13,1,gpt-5-mini,2025-12-01T13:13:38.442918,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",135493,36061.53,12,1,0,0,12,1,,34197.48,1864.05,13
2025_12_01_13_13,2,gpt-5-mini,2025-12-01T13:14:16.818710,False,2,23,0,,37454,25890.01,5,1,1,0,4,1,0,25349.32,540.69,6
2025_12_01_13_13,3,gpt-5-mini,2025-12-01T13:14:44.724250,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",119109,38565.1,11,1,0,0,11,1,,36784.18,1780.92,12
2025_12_01_13_13,4,gpt-5-mini,2025-12-01T13:15:30.504054,False,2,23,0,,36907,26448.99,5,1,1,0,4,1,"Model name is 'allenai/OLMo-7B', expected one of ['OLMo 7B', 'OLMo-7B']",26010.57,438.42,6
2025_12_01_13_13,5,gpt-5-mini,2025-12-01T13:15:59.605005,False,2,23,0,,38292,31239.35,5,1,1,0,4,1,"Model name is 'allenai/OLMo-7B', expected one of ['OLMo 7B', 'OLMo-7B']",30806.63,432.72,6
2025_12_01_13_13,6,gpt-5-mini,2025-12-01T13:16:38.834416,False,4,23,0,,27104,19010.91,4,1,1,0,3,1,"Task type is 'None', expected 'text-generation'",18627.29,383.62,5
2025_12_01_13_13,7,gpt-5-mini,2025-12-01T13:16:59.120465,False,2,23,0,,28017,28726.83,4,1,1,0,3,1,"Model name is 'OLMo 7B evaluations (reported)', expected one of ['OLMo 7B', 'OLMo-7B']",28155.37,571.46,5
2025_12_01_13_13,8,gpt-5-mini,2025-12-01T13:17:34.961679,False,2,23,0,,46900,33092.78,7,1,1,0,6,1,"Model name is 'allenai/OLMo-7B', expected one of ['OLMo 7B', 'OLMo-7B']",32599.29,493.49,8
2025_12_01_13_13,9,gpt-5-mini,2025-12-01T13:18:10.306294,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",34536,16857.33,5,0,0,0,5,0,,15739.75,1117.58,6
2025_12_01_13_13,10,gpt-5-mini,2025-12-01T13:18:34.280676,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",30514,21887.64,5,0,0,0,5,0,,20902.48,985.16,6
