batch_id,run_number,model,timestamp,passed,assertions_passed,assertions_total,metrics_count,benchmarks_found,tokens,conversation_span_ms,tool_calls,tool_errors,mcp_calls,mcp_errors,execute_calls,execute_errors,error_message,llm_time_ms,tool_time_ms,turns
2025_12_01_11_55,1,moonshotai/Kimi-K2-Instruct-0905,2025-12-01T11:55:41.073233,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",65064,13155.41,6,0,1,0,5,0,,11864.85,1290.56,7
2025_12_01_11_55,2,moonshotai/Kimi-K2-Instruct-0905,2025-12-01T11:55:55.479037,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",87851,12136.43,8,0,1,0,7,0,,10220.84,1915.59,9
2025_12_01_11_55,3,moonshotai/Kimi-K2-Instruct-0905,2025-12-01T11:56:09.848595,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",48101,6915.93,6,0,0,0,6,0,,5912.92,1003.01,7
2025_12_01_11_55,4,moonshotai/Kimi-K2-Instruct-0905,2025-12-01T11:56:23.106967,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",77370,12556.45,7,0,1,0,6,0,,10744.13,1812.32,8
2025_12_01_11_55,5,moonshotai/Kimi-K2-Instruct-0905,2025-12-01T11:56:36.885282,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",69584,9588.07,7,1,2,1,5,0,,8168.78,1419.29,8
2025_12_01_11_55,6,moonshotai/Kimi-K2-Instruct-0905,2025-12-01T11:56:53.662499,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",57588,8215.85,7,0,0,0,7,0,,6744.52,1471.33,8
2025_12_01_11_55,7,moonshotai/Kimi-K2-Instruct-0905,2025-12-01T11:57:03.209208,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",40055,7985.78,5,0,0,0,5,0,,6872.69,1113.09,6
2025_12_01_11_55,8,moonshotai/Kimi-K2-Instruct-0905,2025-12-01T11:57:13.552147,False,6,23,9,,111277,25980.1,9,1,1,0,8,1,"Metrics list has 9 entries, expected at least 11",24536.2,1443.9,10
2025_12_01_11_55,9,moonshotai/Kimi-K2-Instruct-0905,2025-12-01T11:57:40.823087,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",77557,12563.8,7,0,1,0,6,0,,10477.9,2085.9,8
2025_12_01_11_55,10,moonshotai/Kimi-K2-Instruct-0905,2025-12-01T11:57:55.722846,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",63301,8401.88,6,0,1,0,5,0,,7132.75,1269.13,7
2025_12_01_11_58,1,openai/gpt-oss-120b,2025-12-01T11:58:12.977378,False,11,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",98825,68573.5,6,0,0,0,6,0,"Metric arc_challenge has value 44.5, expected 48.5",7042.65,61530.85,7
2025_12_01_11_58,2,openai/gpt-oss-120b,2025-12-01T11:59:25.769204,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",44251,3477.16,4,0,0,0,4,0,,3265.67,211.49,5
2025_12_01_11_58,3,openai/gpt-oss-120b,2025-12-01T11:59:30.528197,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",37826,5177.83,6,1,0,0,6,1,,4154.44,1023.39,7
2025_12_01_11_58,4,openai/gpt-oss-120b,2025-12-01T11:59:37.037969,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",38974,8310.67,6,1,0,0,6,1,,6746.89,1563.78,7
2025_12_01_11_58,5,openai/gpt-oss-120b,2025-12-01T11:59:46.537124,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",36981,7084.35,6,0,0,0,6,0,,6042.79,1041.56,7
2025_12_01_11_58,6,openai/gpt-oss-120b,2025-12-01T11:59:54.840483,False,0,23,0,,62175,5379.57,4,0,1,0,3,0,Output file 'olmo_7b_evaluations.yaml' not found,4879.92,499.65,5
2025_12_01_11_58,7,openai/gpt-oss-120b,2025-12-01T12:00:02.332120,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",43189,3415.62,4,0,0,0,4,0,,3212.47,203.15,5
2025_12_01_11_58,8,openai/gpt-oss-120b,2025-12-01T12:00:11.996958,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",70646,9461.17,8,1,2,1,6,0,,7637.68,1823.49,9
2025_12_01_11_58,9,openai/gpt-oss-120b,2025-12-01T12:00:22.788395,False,0,23,0,,40652,4311.83,3,0,1,0,2,0,Output file 'olmo_7b_evaluations.yaml' not found,3915.84,395.99,4
2025_12_01_11_58,10,openai/gpt-oss-120b,2025-12-01T12:00:28.339920,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",107239,13025.52,11,1,5,1,6,0,,10696.91,2328.61,12
2025_12_01_12_01,1,gpt-5-mini,2025-12-01T12:01:07.712965,False,2,23,0,,36599,32118.36,5,1,1,0,4,1,"Model name is 'allenai/OLMo-7B', expected one of ['OLMo 7B', 'OLMo-7B']",31578.23,540.13,6
2025_12_01_12_01,2,gpt-5-mini,2025-12-01T12:01:47.091693,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",50569,30565.29,9,2,0,0,9,2,,29384.55,1180.74,10
2025_12_01_12_01,3,gpt-5-mini,2025-12-01T12:02:19.807978,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",50742,31140.48,9,2,0,0,9,2,,29962.07,1178.41,10
2025_12_01_12_01,4,gpt-5-mini,2025-12-01T12:02:58.064595,False,2,23,0,,37211,42455.32,5,1,1,0,4,1,0,41899.06,556.26,6
2025_12_01_12_01,5,gpt-5-mini,2025-12-01T12:03:42.655441,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",23913,20113.72,4,0,0,0,4,0,,19055.3,1058.42,5
2025_12_01_12_01,6,gpt-5-mini,2025-12-01T12:04:09.856599,False,6,23,1,,33130,48736.5,4,0,1,0,3,0,"Metrics list has 1 entries, expected at least 11",48364.76,371.74,5
2025_12_01_12_01,7,gpt-5-mini,2025-12-01T12:05:01.590923,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",34176,24802.25,5,0,0,0,5,0,,23795.95,1006.3,6
2025_12_01_12_01,8,gpt-5-mini,2025-12-01T12:05:33.571220,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",30507,24739.82,5,0,0,0,5,0,,23722.44,1017.38,6
2025_12_01_12_01,9,gpt-5-mini,2025-12-01T12:05:59.581490,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",151349,43460.99,12,0,0,0,12,0,,41541.23,1919.76,13
2025_12_01_12_01,10,gpt-5-mini,2025-12-01T12:06:51.890882,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",45223,22720.52,8,1,0,0,8,1,,21607.41,1113.11,9
2025_12_01_12_09,1,claude-haiku-4-5,2025-12-01T12:09:57.227814,False,11,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",51166,21328.42,5,0,0,0,5,0,"Metric arc_challenge has value 44.5, expected 48.5",20258.88,1069.54,6
2025_12_01_12_09,2,claude-haiku-4-5,2025-12-01T12:10:19.869115,False,11,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",40833,19550.31,4,0,0,0,4,0,"Metric arc_challenge has value 44.5, expected 48.5",18516.08,1034.23,5
2025_12_01_12_09,3,claude-haiku-4-5,2025-12-01T12:10:41.504131,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",40281,16591.97,4,0,0,0,4,0,,15600.93,991.04,5
2025_12_01_12_09,4,claude-haiku-4-5,2025-12-01T12:10:59.348780,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",52088,20590.0,5,0,0,0,5,0,,19534.23,1055.77,6
2025_12_01_12_09,5,claude-haiku-4-5,2025-12-01T12:11:23.135751,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",53333,25484.92,5,0,0,0,5,0,,24336.05,1148.87,6
2025_12_01_12_09,6,claude-haiku-4-5,2025-12-01T12:11:50.787090,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",40327,18650.47,4,0,0,0,4,0,,16502.24,2148.23,5
2025_12_01_12_09,7,claude-haiku-4-5,2025-12-01T12:12:11.809892,False,11,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",49702,21181.28,5,0,0,0,5,0,"Metric arc_challenge has value 44.5, expected 48.5",20050.0,1131.28,6
2025_12_01_12_09,8,claude-haiku-4-5,2025-12-01T12:12:34.208533,False,11,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",40808,15655.43,4,0,0,0,4,0,"Metric arc_challenge has value 44.5, expected 48.5",14631.75,1023.68,5
2025_12_01_12_09,9,claude-haiku-4-5,2025-12-01T12:12:52.146827,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",111714,28334.6,8,0,1,0,7,0,,26516.15,1818.45,9
2025_12_01_12_09,10,claude-haiku-4-5,2025-12-01T12:13:22.646862,True,23,23,11,"arc_challenge,arc_easy,boolq,copa,hellaswag,mmlu,openbookqa,piqa,sciq,truthfulqa,winogrande",80492,26983.41,6,0,1,0,5,0,,24202.43,2780.98,7
